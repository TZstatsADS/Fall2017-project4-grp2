---
title: "Project 4 - Collaborative Filtering"
author: "Yufei Zhao, Joaquim Lyrio"
date: "3/22/2017"
output: pdf_document
---
In this project, we are implementing different collaborative filtering algorithms and comparing their performances. There are basically two types of algorithms: memory-based and model-based algorithms.

# MEMORY-BASED ALGORITHMS

## Step 0: Load the packages, specify directories

First, let's load (and install, if necessary) all the required packages to run the code.
```{r}
if (!require("data.table")) install.packages("data.table")
if (!require("entropy")) install.packages("entropy")
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
library(entropy)
library(data.table)
```

Now, let's source all the R functions necessary to run the code.
```{r}
# load axiliar functions
source("../lib/auxFunctions.R")
```

## Step 1: Load and process the data

First, we read Microsoft's data set and convert it to the format with features in columns and users in rows, so that it is easy to perform calculations.
```{r}
data_orig <- readAnonymousMicrosoftWebData()
data_MS   <- convertToMatrixData1( data = data_orig )
```

The training and test sets are printed to csv files. This way, we will just read them.
```{r}
data_train_MS <- read.csv("../data/data_sample/MS_sample/matrix_train.csv", header = TRUE)
data_test_MS  <- read.csv("../data/data_sample/MS_sample/matrix_test.csv",header=TRUE)
```

Additionally, we read the movie data set and convert it to the same format.
```{r}
test_path <- "../data/data_sample/eachmovie_sample/data_test.csv"
train_path <- "../data/data_sample/eachmovie_sample/data_train.csv"
combo <- c(test_path, train_path)
for (i in 1:2){
  convert_matrix_from_dataset(path = combo[i])
}
data_train_movie <- read.csv("../data/data_sample/eachmovie_sample/movie_train.csv", header = TRUE)
data_test_movie <- read.csv("../data/data_sample/eachmovie_sample/movie_test.csv",header=TRUE)
```


## Step 2 : Calculate similarity measures

The computation of the Spearman correlation and vector similarity measures are computed by the Java code (./lib/SimilarityMeasures.java) and a csv file with the results is outputted for each data set. To run such, follow instructions written in README.md file present in './lib'. 

Since the code is very computationally intensive, we will just load the results. Furthermore, since the two matrices are too heavy (around 500MB and 700MB), we cannot upload them into GitHub. This way, we will read them from Dropbox (https://www.dropbox.com/sh/pjqik8k8kexs80n/AAA5GmRCPsxcbTvLPW2ClcEWa?dl=0). 

Let's read both data sets then.
```{r}
# read microsoft similarity measures
similarity_measures_MS <- read.csv("~/Dropbox/Project4/similarity_measures_MS.csv", header=TRUE)

# read movie similarity measures
similarity_measures_movie <- read.csv("~/Dropbox/Project4/similarity_measures_eachmovie.csv", header=TRUE)
```

The entropy similarity measure was computed in R, by the function 'calculateEntropy', present in ./lib/auxFunctions.R.
```{r}
entropyMS    <- calculateEntropy( data_train_MS )
entropyMovie <- calculateEntropy( data_train_movie )
```

## Step 3: variance weighting
Now, let's calculate the variance weighting for both data sets.

Microsoft
```{r}
var_w_MS <- matrix( NA, ncol = 3 )
colnames(var_w_MS) <- c("i","j","vw")
iRow <- 0
for( user_a in row.names(data_train_MS) ){
  
  for( user_u in row.names(data_train_MS) ){
    
    var_w_MS[iRow, 1] <- user_a
    var_w_MS[iRow, 2] <- user_u
    var_w_MS[iRow, 3] <- varianceWeighting( data_train_MS, user_a, user_u )
    
    iRow <- iRow + 1 
  }
}
```

EachMovie
```{r}
var_w_movie<- matrix( NA, ncol = 3 )
colnames(var_w_movie) <- c("i","j","vw")
iRow <- 0
for( user_a in row.names(data_train_movie) ){
  
  for( user_u in row.names(data_train_movie) ){
    
    var_w_movie[iRow, 1] <- user_a
    var_w_movie[iRow, 2] <- user_u
    var_w_movie[iRow, 3] <- varianceWeighting( data_train_movie, user_a, user_u )
    
    iRow <- iRow + 1 
  }
}

```

## Step 4: Computing neighbors

Now that we have calculated the similarity measures between users on both data sets, we can compute the various neighbor methods. Let's do it for both data sets.

### Microsoft: selecting neighbors (top, threshold, combo)

Best-n-estimator (n = 20)
```{r}
# make neighborhood selection based on top 20 in terms of spearman correlation and vector similarity
neighbor_select_top_MS(similarity_measures_MS, 20)
```

Weight Threshold (threshold = 0.3)
```{r}
# make neighborhood selection based on threshold 0.3 in terms of spearman correlation and vector similarity
neighbor_select_threshold_MS(similarity_measures_MS, 0.3)
```

Combined (n = 20 and threshold = 0.1)
```{r}
# make neighborhood selection based on top 20 and threshold 0.1 in terms of spearman correlation and vector similarity
neighbor_select_combo_MS(similarity_measures_MS,20,0.1)
```


### Movie: selecting neighbors (top, threshold, combo)

Best-n-estimator (n = 20)
```{r}
# make neighborhood selection based on top 20 in terms of spearman correlation and vector similarity
neighbor_select_top_movie(similarity_measures_movie,20)
```

Weight Threshold (threshold = 0.3)
```{r}
# make neighborhood selection based on threshold 0.3 in terms of spearman correlation and vector similarity
neighbor_select_threshold_movie(similarity_measures_movie,0.3)
```

Combined (n = 20 and threshold = 0.1)
```{r}
# make neighborhood selection based on top 20 and threshold 0.1 in terms of spearman correlation and vector similarity
neighbor_select_combo_movie(similarity_measures_movie,20,0.1)
```


##Step 6: make predictions
Now that we have calculated the different types of measures of similarity and the neighbors according to them, we can move on to the prediction step.



##Step 7: display results




# MODEL-BASED ALGORITHM

### (TIAN TIAN)
